{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Meta-World\n",
    "\n",
    " ## Introduction\n",
    "\n",
    " This tutorial demonstrates how to use the `trace` package to optimize a policy for a simulated robot performing a pick-and-place task.\n",
    "\n",
    " ## Setup and Installation\n",
    "\n",
    " This example requires [LLF-Bench](https://github.com/microsoft/LLF-Bench) in addition to `trace`. You can install them as follows\n",
    "\n",
    "     git clone https://github.com/microsoft/LLF-Bench.git\n",
    "     cd LLF-Bench\n",
    "     pip install -e .[metaworld]\n",
    "\n",
    " Let's start by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import config_list_from_json\n",
    "import llfbench\n",
    "import random\n",
    "import numpy as np\n",
    "import opto.trace as trace\n",
    "from opto.optimizers import OptoPrime\n",
    "from opto.trace.bundle import ExceptionNode\n",
    "from opto.trace.errors import ExecutionError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Environment Setup\n",
    "\n",
    " Define the environment and helper functions to parse observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_obs(obs):\n",
    "    \"\"\"Parse the observation string into a dictionary of lists of floats.\"\"\"\n",
    "    import json\n",
    "\n",
    "    obs = json.loads(obs)\n",
    "    for key in obs:\n",
    "        obs[key] = obs[key].replace(\"[\", \"\").replace(\"]\", \"\").split()\n",
    "        obs[key] = [float(i) for i in obs[key]]\n",
    "    return obs\n",
    "\n",
    "class TracedEnv:\n",
    "    def __init__(self, env_name, seed=0, relative=True):\n",
    "        self.seed = seed\n",
    "        self.env_name = env_name\n",
    "        self.relative = relative\n",
    "        self.init()\n",
    "\n",
    "    def init(self):\n",
    "        random.seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        self.env = llfbench.make(self.env_name)\n",
    "        self.env.reset(seed=self.seed)\n",
    "        self.env.action_space.seed(self.seed)\n",
    "        self.env.control_mode(\"relative\" if self.relative else \"absolute\")\n",
    "        self.obs = None\n",
    "\n",
    "    @trace.bundle()\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment and return the initial observation and info.\n",
    "        \"\"\"\n",
    "        obs, info = self.env.reset()\n",
    "        obs[\"observation\"] = parse_obs(obs[\"observation\"])\n",
    "        self.obs = obs\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        try:\n",
    "            control = action.data if isinstance(action, trace.Node) else action\n",
    "            next_obs, reward, termination, truncation, info = self.env.step(control)\n",
    "            next_obs[\"observation\"] = parse_obs(next_obs[\"observation\"])\n",
    "            self.obs = next_obs\n",
    "        except Exception as e:\n",
    "            e_node = ExceptionNode(\n",
    "                e,\n",
    "                inputs={\"action\": action},\n",
    "                description=\"[exception] The operator step raises an exception.\",\n",
    "                name=\"exception_step\",\n",
    "            )\n",
    "            raise ExecutionError(e_node)\n",
    "\n",
    "        @trace.bundle()\n",
    "        def step(action):\n",
    "            \"\"\"\n",
    "            Take action in the environment and return the next observation\n",
    "            \"\"\"\n",
    "            return next_obs\n",
    "\n",
    "        next_obs = step(action)\n",
    "        return next_obs, reward, termination, truncation, info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Rollout Function\n",
    "\n",
    " Define a function to perform a rollout using the current policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(env, horizon, controller):\n",
    "    \"\"\"Rollout a controller in an env for horizon steps.\"\"\"\n",
    "    traj = dict(observation=[], action=[], reward=[], termination=[], truncation=[], success=[], info=[])\n",
    "    obs, info = env.reset()\n",
    "    traj[\"observation\"].append(obs)\n",
    "\n",
    "    for t in range(horizon):\n",
    "        controller_input = obs[\"observation\"]\n",
    "        error = None\n",
    "        try:\n",
    "            action = controller(controller_input)\n",
    "            next_obs, reward, termination, truncation, info = env.step(action)\n",
    "        except trace.ExecutionError as e:\n",
    "            error = e\n",
    "            break\n",
    "\n",
    "        if error is None:\n",
    "            traj[\"observation\"].append(next_obs)\n",
    "            traj[\"action\"].append(action)\n",
    "            traj[\"reward\"].append(reward)\n",
    "            traj[\"termination\"].append(termination)\n",
    "            traj[\"truncation\"].append(truncation)\n",
    "            traj[\"success\"].append(info[\"success\"])\n",
    "            traj[\"info\"].append(info)\n",
    "            if termination or truncation or info[\"success\"]:\n",
    "                break\n",
    "            obs = next_obs\n",
    "    return traj, error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Optimize using Trace\n",
    "\n",
    " Define the function to optimize the policy using the Trace package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_policy(\n",
    "    env_name,\n",
    "    horizon,\n",
    "    n_episodes=10,\n",
    "    n_optimization_steps=100,\n",
    "    seed=0,\n",
    "    relative=True,\n",
    "    verbose=False,\n",
    "    model=\"gpt-4-0125-preview\",\n",
    "):\n",
    "\n",
    "    @trace.bundle(trainable=True)\n",
    "    def controller(obs):\n",
    "        \"\"\"\n",
    "        A feedback controller that computes the action based on the observation.\n",
    "\n",
    "        Args:\n",
    "            obs: (dict) The observation from the environment. Each key is a string (indicating a type of observation) and the value is a list of floats.\n",
    "        Output:\n",
    "            action: (list or nd.array) A 4-dimensional vector.\n",
    "        \"\"\"\n",
    "        return [0, 0, 0, 0]\n",
    "\n",
    "    config_list = config_list_from_json(\"OAI_CONFIG_LIST\")\n",
    "    config_list = [config for config in config_list if config[\"model\"] == model]\n",
    "    optimizer = OptoPrime(controller.parameters(), config_list=config_list)\n",
    "\n",
    "    env = TracedEnv(env_name, seed=seed, relative=relative)\n",
    "\n",
    "    successes = []\n",
    "    returns = []\n",
    "    print(\"Optimization Starts\")\n",
    "    for i in range(n_optimization_steps):\n",
    "        env.init()\n",
    "        traj, error = rollout(env, horizon, controller)\n",
    "\n",
    "        if error is None:\n",
    "            feedback = f\"Success: {traj['success'][-1]}\\nReturn: {sum(traj['reward'])}\"\n",
    "            target = traj[\"observation\"][-1][\"observation\"]\n",
    "\n",
    "            successes.append(traj[\"success\"][-1])\n",
    "            returns.append(sum(traj[\"reward\"]))\n",
    "        else:\n",
    "            feedback = error.exception_node.create_feedback()\n",
    "            target = error.exception_node\n",
    "\n",
    "        # Add instruction from the LLFbench environment, which contains\n",
    "        # information about the action space and problem background. The original instruction says\n",
    "        # obsrvaiton is a json string. But here we've parsed it as a dict so we\n",
    "        # update the instruction.\n",
    "        instruction = traj[\"observation\"][0][\"instruction\"].data\n",
    "        infix = \"You will get observations of the robot state \"\n",
    "        prefix, suffix = instruction.split(infix)\n",
    "        keys = \", \".join(traj[\"observation\"][0][\"observation\"].data.keys())\n",
    "        suffix = suffix.replace(\"json strings.\", f\"dict, where the keys are {keys}.\")\n",
    "\n",
    "        # Add an task specific explanation; as the original instruction says\n",
    "        # only it's a pick-place task, which is too vague. We clarify the task.\n",
    "        assert env_name in [\"llf-metaworld-pick-place-v2\", \"llf-metaworld-reach-v2\"]\n",
    "        if env_name == \"llf-metaworld-pick-place-v2\":\n",
    "            hint = prefix + \"The goal of the task is to pick up a puck and put it to a goal position. \" + infix + suffix\n",
    "        else:\n",
    "            hint = prefix + infix + suffix\n",
    "\n",
    "        optimizer.objective = hint + optimizer.default_objective\n",
    "\n",
    "        optimizer.zero_feedback()\n",
    "        optimizer.backward(target, feedback)\n",
    "        optimizer.step(verbose=verbose)\n",
    "\n",
    "        print(f\"Iteration: {i}, Feedback: {feedback}, Parameter: {controller.parameter.data}\")\n",
    "\n",
    "    successes.append(traj[\"success\"][-1])\n",
    "    returns.append(sum(traj[\"reward\"]))\n",
    "    print(\"Final Returns:\", sum(traj[\"reward\"]))\n",
    "    return successes, returns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Execute the Optimization Process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chinganc/miniconda3/envs/trace/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:32: UserWarning: \u001b[33mWARN: A Box observation space maximum and minimum values are equal. Actual equal coordinates: [(36,), (37,), (38,)]\u001b[0m\n",
      "  logger.warn(\n",
      "/home/chinganc/miniconda3/envs/trace/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "/home/chinganc/miniconda3/envs/trace/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:131: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting a numpy array, actual type: <class 'str'>\u001b[0m\n",
      "  logger.warn(\n",
      "/home/chinganc/miniconda3/envs/trace/lib/python3.8/site-packages/gymnasium/spaces/box.py:240: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  gym.logger.warn(\"Casting input x to numpy array.\")\n",
      "/home/chinganc/miniconda3/envs/trace/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.control_mode to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.control_mode` for environment variables or `env.get_wrapper_attr('control_mode')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Starts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chinganc/miniconda3/envs/trace/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "/home/chinganc/miniconda3/envs/trace/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:131: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting a numpy array, actual type: <class 'str'>\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "successes, returns = optimize_policy(\n",
    "    env_name=\"llf-metaworld-pick-place-v2\",\n",
    "    horizon=10,\n",
    "    n_episodes=10,\n",
    "    n_optimization_steps=30,\n",
    "    seed=2,\n",
    "    relative=True,\n",
    "    verbose='output',\n",
    "    model=\"gpt-4-0125-preview\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot successes, returns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(successes)\n",
    "plt.xlabel(\"Optimization Steps\")\n",
    "plt.ylabel(\"Success\")\n",
    "plt.title(\"Successes\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(returns)\n",
    "plt.xlabel(\"Optimization Steps\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.title(\"Returns\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This completes the tutorial on using the Trace package for optimizing codes in a multi-step RL environment. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
