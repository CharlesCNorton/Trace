{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta-World\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial demonstrates how to use the `trace` package to optimize a policy for a simulated robot performing a pick-and-place task.\n",
    "\n",
    "## Setup and Installation\n",
    "\n",
    "This example requires [LLF-Bench](https://github.com/microsoft/LLF-Bench) in addition to `trace`. You can install them as follows\n",
    "\n",
    "    git clone https://github.com/microsoft/LLF-Bench.git\n",
    "    cd LLF-Bench\n",
    "    pip install -e .[metaworld]\n",
    "\n",
    "Let's start by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import config_list_from_json\n",
    "import llfbench\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import opto.trace as trace\n",
    "from opto.optimizers import OptoPrime\n",
    "from opto.trace.bundle import ExceptionNode\n",
    "from opto.trace.errors import ExecutionError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Define the environment and helper functions to parse observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_obs(obs):\n",
    "    \"\"\"Parse the observation string into a dictionary of lists of floats.\"\"\"\n",
    "    import json\n",
    "\n",
    "    obs = json.loads(obs)\n",
    "    for key in obs:\n",
    "        obs[key] = obs[key].replace(\"[\", \"\").replace(\"]\", \"\").split()\n",
    "        obs[key] = [float(i) for i in obs[key]]\n",
    "    return obs\n",
    "\n",
    "class TracedEnv:\n",
    "    def __init__(self, env_name, seed=0, relative=True):\n",
    "        self.seed = seed\n",
    "        self.env_name = env_name\n",
    "        self.relative = relative\n",
    "        self.init()\n",
    "\n",
    "    def init(self):\n",
    "        random.seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        self.env = llfbench.make(self.env_name)\n",
    "        self.env.reset(seed=self.seed)\n",
    "        self.env.action_space.seed(self.seed)\n",
    "        self.env.control_mode(\"relative\" if self.relative else \"absolute\")\n",
    "        self.obs = None\n",
    "\n",
    "    @trace.bundle()\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment and return the initial observation.\"\"\"\n",
    "        obs, info = self.env.reset()\n",
    "        obs[\"observation\"] = parse_obs(obs[\"observation\"])\n",
    "        self.obs = obs\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        try:\n",
    "            control = action.data if isinstance(action, trace.Node) else action\n",
    "            next_obs, reward, termination, truncation, info = self.env.step(control)\n",
    "            next_obs[\"observation\"] = parse_obs(next_obs[\"observation\"])\n",
    "            self.obs = next_obs\n",
    "        except Exception as e:\n",
    "            e_node = ExceptionNode(\n",
    "                e,\n",
    "                inputs={\"action\": action},\n",
    "                description=\"[exception] The operator step raises an exception.\",\n",
    "                name=\"exception_step\",\n",
    "            )\n",
    "            raise ExecutionError(e_node)\n",
    "\n",
    "        @trace.bundle()\n",
    "        def step(action):\n",
    "            return next_obs\n",
    "\n",
    "        next_obs = step(action)\n",
    "        return next_obs, reward, termination, truncation, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rollout Function\n",
    "\n",
    "Define a function to perform a rollout using the current policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(env, horizon, controller):\n",
    "    \"\"\"Rollout a controller in an env for horizon steps.\"\"\"\n",
    "    traj = dict(observation=[], action=[], reward=[], termination=[], truncation=[], success=[], info=[])\n",
    "    obs, info = env.reset()\n",
    "    traj[\"observation\"].append(obs)\n",
    "\n",
    "    for t in range(horizon):\n",
    "        controller_input = obs[\"observation\"]\n",
    "        error = None\n",
    "        try:\n",
    "            action = controller(controller_input)\n",
    "            next_obs, reward, termination, truncation, info = env.step(action)\n",
    "        except ExecutionError as e:\n",
    "            error = e\n",
    "            break\n",
    "\n",
    "        if error is None:\n",
    "            traj[\"observation\"].append(next_obs)\n",
    "            traj[\"action\"].append(action)\n",
    "            traj[\"reward\"].append(reward)\n",
    "            traj[\"termination\"].append(termination)\n",
    "            traj[\"truncation\"].append(truncation)\n",
    "            traj[\"success\"].append(info[\"success\"])\n",
    "            traj[\"info\"].append(info)\n",
    "            if termination or truncation or info[\"success\"]:\n",
    "                break\n",
    "            obs = next_obs\n",
    "    return traj, error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize using Trace\n",
    "\n",
    "Define the function to optimize the policy using the Trace package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_policy(\n",
    "    env_name,\n",
    "    horizon,\n",
    "    n_episodes=10,\n",
    "    n_optimization_steps=100,\n",
    "    seed=0,\n",
    "    relative=True,\n",
    "    feedback_type=\"a\",\n",
    "    verbose=False,\n",
    "    model=\"gpt-4-0125-preview\",\n",
    "):\n",
    "\n",
    "    @trace.bundle(trainable=True)\n",
    "    def controller(obs):\n",
    "        \"\"\"A feedback controller that computes the action based on the observation.\"\"\"\n",
    "        return [0, 0, 0, 0]\n",
    "\n",
    "    optimizer = OptoPrime(controller.parameters(), config_list=config_list_from_json(\"OAI_CONFIG_LIST\"))\n",
    "\n",
    "    env = TracedEnv(env_name, seed=seed, relative=relative)\n",
    "\n",
    "    print(\"Optimization Starts\")\n",
    "    for i in range(n_optimization_steps):\n",
    "        env.init()\n",
    "        traj, error = rollout(env, horizon, controller)\n",
    "\n",
    "        if error is None:\n",
    "            feedback = f\"Success: {traj['success'][-1]}\\nReturn: {sum(traj['reward'])}\"\n",
    "            target = traj[\"observation\"][-1][\"observation\"]\n",
    "            returns = [sum(traj[\"reward\"]) for _ in range(n_episodes)]\n",
    "        else:\n",
    "            target = error.exception_node\n",
    "            feedback = target.data\n",
    "\n",
    "        optimizer.objective = f\"The goal is to optimize the pick-and-place task. {optimizer.default_objective}\"\n",
    "        optimizer.zero_feedback()\n",
    "        optimizer.backward(target, feedback)\n",
    "        optimizer.step(verbose=verbose)\n",
    "\n",
    "        print(f\"Iteration: {i}, Feedback: {feedback}, Parameter: {controller.parameter.data}\")\n",
    "\n",
    "    returns = [sum(traj[\"reward\"]) for _ in range(n_episodes)]\n",
    "    print(\"Final Returns:\", returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the Optimization Process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_policy(\n",
    "    env_name=\"llf-metaworld-pick-place-v2\",\n",
    "    horizon=10,\n",
    "    n_episodes=10,\n",
    "    n_optimization_steps=100,\n",
    "    seed=0,\n",
    "    relative=True,\n",
    "    feedback_type=\"a\",\n",
    "    verbose=True,\n",
    "    model=\"gpt-4-0125-preview\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This completes the tutorial on using the Trace package for optimizing codes in a multi-step RL environment. Happy optimizing!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
